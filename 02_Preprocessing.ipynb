{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a92ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cleaned data...\n",
      "Cleaned data shape: (36429, 7)\n",
      "Loading validation data...\n",
      "Validation data shape: (4956, 5)\n",
      "\n",
      "=== CHECKING FOR NaN VALUES BEFORE PREPROCESSING ===\n",
      "Training data NaN values:\n",
      "label               0\n",
      "title               0\n",
      "text                0\n",
      "subject             0\n",
      "date            16640\n",
      "title_length        0\n",
      "text_length         0\n",
      "dtype: int64\n",
      "\n",
      "Validation data NaN values:\n",
      "label      0\n",
      "title      0\n",
      "text       0\n",
      "subject    0\n",
      "date       0\n",
      "dtype: int64\n",
      "Validation duplicates: 8\n",
      "\n",
      "=== PREPROCESSING TRAINING DATA ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing titles:   1%|          | 186/36429 [00:00<00:19, 1837.71it/s]c:\\Users\\Amin\\Documents\\Ironhack_projects\\project-nlp-challenge\\utils\\init.py:26: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n",
      "Processing titles:  80%|███████▉  | 29043/36429 [00:10<00:02, 2622.83it/s]c:\\Users\\Amin\\Documents\\Ironhack_projects\\project-nlp-challenge\\utils\\init.py:26: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n",
      "Processing titles: 100%|██████████| 36429/36429 [00:13<00:00, 2631.11it/s]\n",
      "Processing text:  16%|█▌        | 5697/36429 [00:12<01:09, 444.35it/s]c:\\Users\\Amin\\Documents\\Ironhack_projects\\project-nlp-challenge\\utils\\init.py:26: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n",
      "Processing text:  80%|███████▉  | 29135/36429 [01:03<00:18, 396.01it/s]c:\\Users\\Amin\\Documents\\Ironhack_projects\\project-nlp-challenge\\utils\\init.py:26: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n",
      "Processing text: 100%|██████████| 36429/36429 [01:16<00:00, 473.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PREPROCESSING VALIDATION DATA ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation titles:  32%|███▏      | 1577/4956 [00:00<00:01, 2560.02it/s]c:\\Users\\Amin\\Documents\\Ironhack_projects\\project-nlp-challenge\\utils\\init.py:26: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n",
      "Processing validation titles:  63%|██████▎   | 3136/4956 [00:01<00:00, 2568.44it/s]c:\\Users\\Amin\\Documents\\Ironhack_projects\\project-nlp-challenge\\utils\\init.py:26: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n",
      "Processing validation titles: 100%|██████████| 4956/4956 [00:01<00:00, 2575.93it/s]\n",
      "Processing validation text:  30%|███       | 1496/4956 [00:03<00:07, 457.33it/s]c:\\Users\\Amin\\Documents\\Ironhack_projects\\project-nlp-challenge\\utils\\init.py:26: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n",
      "Processing validation text:  67%|██████▋   | 3313/4956 [00:06<00:03, 470.26it/s]c:\\Users\\Amin\\Documents\\Ironhack_projects\\project-nlp-challenge\\utils\\init.py:26: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n",
      "Processing validation text: 100%|██████████| 4956/4956 [00:13<00:00, 371.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CHECKING FOR NaN VALUES AFTER PREPROCESSING ===\n",
      "Training data NaN values:\n",
      "title_clean    0\n",
      "text_clean     0\n",
      "dtype: int64\n",
      "\n",
      "Validation data NaN values:\n",
      "title_clean    0\n",
      "text_clean     0\n",
      "dtype: int64\n",
      "\n",
      "=== CHECKING FOR EMPTY STRINGS ===\n",
      "Empty title_clean in train: 5\n",
      "Empty text_clean in train: 494\n",
      "Empty title_clean in val: 2\n",
      "Empty text_clean in val: 23\n",
      "Processed training data saved to: c:\\Users\\Amin\\Documents\\Ironhack_projects\\project-nlp-challenge\\processed\\train_processed.csv\n",
      "Processed validation data saved to: c:\\Users\\Amin\\Documents\\Ironhack_projects\\project-nlp-challenge\\processed\\val_processed.csv\n"
     ]
    }
   ],
   "source": [
    "# 02_Preprocessing.ipynb\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from config import *\n",
    "from utils.init import *\n",
    "\n",
    "# Execution guard to prevent double execution\n",
    "if 'preprocessing_executed' not in globals():\n",
    "    preprocessing_executed = True\n",
    "    \n",
    "    # Load cleaned data\n",
    "    print(\"Loading cleaned data...\")\n",
    "    try:\n",
    "        df = pd.read_csv(CLEANED_DATA_PATH)\n",
    "        print(f\"Cleaned data shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading cleaned data: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Load validation data\n",
    "    print(\"Loading validation data...\")\n",
    "    try:\n",
    "        val_df = pd.read_csv(VALIDATION_DATA_PATH)\n",
    "        print(f\"Validation data shape: {val_df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading validation data: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Check for NaN values BEFORE preprocessing\n",
    "    print(\"\\n=== CHECKING FOR NaN VALUES BEFORE PREPROCESSING ===\")\n",
    "    print(\"Training data NaN values:\")\n",
    "    print(df.isna().sum())\n",
    "    print(\"\\nValidation data NaN values:\")\n",
    "    print(val_df.isna().sum())\n",
    "\n",
    "    # Handle NaN values\n",
    "    df = df.fillna('')\n",
    "    val_df = val_df.fillna('')\n",
    "\n",
    "    # Check validation duplicates\n",
    "    val_duplicates = val_df.duplicated().sum()\n",
    "    print(f\"Validation duplicates: {val_duplicates}\")\n",
    "\n",
    "    # Apply preprocessing pipeline\n",
    "    print(\"\\n=== PREPROCESSING TRAINING DATA ===\")\n",
    "    tqdm.pandas(desc=\"Processing titles\")\n",
    "    df['title_clean'] = df['title'].progress_apply(\n",
    "        lambda x: full_clean_pipeline(x, CUSTOM_STOPWORDS)\n",
    "    )\n",
    "\n",
    "    tqdm.pandas(desc=\"Processing text\")\n",
    "    df['text_clean'] = df['text'].progress_apply(\n",
    "        lambda x: full_clean_pipeline(x, CUSTOM_STOPWORDS)\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== PREPROCESSING VALIDATION DATA ===\")\n",
    "    tqdm.pandas(desc=\"Processing validation titles\")\n",
    "    val_df['title_clean'] = val_df['title'].progress_apply(\n",
    "        lambda x: full_clean_pipeline(x, CUSTOM_STOPWORDS)\n",
    "    )\n",
    "\n",
    "    tqdm.pandas(desc=\"Processing validation text\")\n",
    "    val_df['text_clean'] = val_df['text'].progress_apply(\n",
    "        lambda x: full_clean_pipeline(x, CUSTOM_STOPWORDS)\n",
    "    )\n",
    "\n",
    "    # Check for NaN values AFTER preprocessing\n",
    "    print(\"\\n=== CHECKING FOR NaN VALUES AFTER PREPROCESSING ===\")\n",
    "    print(\"Training data NaN values:\")\n",
    "    print(df[['title_clean', 'text_clean']].isna().sum())\n",
    "    print(\"\\nValidation data NaN values:\")\n",
    "    print(val_df[['title_clean', 'text_clean']].isna().sum())\n",
    "\n",
    "    # Handle any remaining NaN values\n",
    "    df['title_clean'] = df['title_clean'].fillna('no content')\n",
    "    df['text_clean'] = df['text_clean'].fillna('no content')\n",
    "    val_df['title_clean'] = val_df['title_clean'].fillna('no content')\n",
    "    val_df['text_clean'] = val_df['text_clean'].fillna('no content')\n",
    "\n",
    "    # Check empty strings\n",
    "    print(\"\\n=== CHECKING FOR EMPTY STRINGS ===\")\n",
    "    print(f\"Empty title_clean in train: {(df['title_clean'] == '').sum()}\")\n",
    "    print(f\"Empty text_clean in train: {(df['text_clean'] == '').sum()}\")\n",
    "    print(f\"Empty title_clean in val: {(val_df['title_clean'] == '').sum()}\")\n",
    "    print(f\"Empty text_clean in val: {(val_df['text_clean'] == '').sum()}\")\n",
    "\n",
    "    # Replace empty strings\n",
    "    df['title_clean'] = df['title_clean'].replace('', 'no content')\n",
    "    df['text_clean'] = df['text_clean'].replace('', 'no content')\n",
    "    val_df['title_clean'] = val_df['title_clean'].replace('', 'no content')\n",
    "    val_df['text_clean'] = val_df['text_clean'].replace('', 'no content')\n",
    "\n",
    "    # Save processed data\n",
    "    try:\n",
    "        df.to_csv(PROCESSED_TRAIN_PATH, index=False)\n",
    "        print(f\"Processed training data saved to: {PROCESSED_TRAIN_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving processed training data: {e}\")\n",
    "        raise\n",
    "\n",
    "    try:\n",
    "        val_df.to_csv(PROCESSED_VAL_PATH, index=False)\n",
    "        print(f\"Processed validation data saved to: {PROCESSED_VAL_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving processed validation data: {e}\")\n",
    "        raise\n",
    "\n",
    "else:\n",
    "    print(\"Preprocessing already executed. Restart kernel to run again.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
